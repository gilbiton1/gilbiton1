Enhancing MobileBERT with Data Augmentation and LoRA Fine-Tuning
This project explores improvements to MobileBERT—a lightweight, task-agnostic BERT model optimized for mobile and resource-constrained environments. Our goal is to boost performance and robustness by applying dynamic data augmentation and efficient fine-tuning using LoRA (Low-Rank Adaptation).

Key Features
🧠 MobileBERT Backbone: Fast and compact transformer model.

🔁 Dynamic Augmentation: Real-time text perturbations (word swaps, synonyms, typos) during training.

🔧 LoRA Fine-Tuning: Efficient weight adaptation without full model retraining.

📊 Multi-Task Evaluation: Supports all GLUE tasks with F1 score tracking.

📱 Mobile-Ready: Ideal for deployment on edge and mobile devices.

Project Goals
Improve generalization, robustness to input variations, and reduce computational costs during training—all while preserving MobileBERT’s speed and compactness.

