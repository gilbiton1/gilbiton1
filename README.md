Enhancing MobileBERT with Data Augmentation and LoRA Fine-Tuning
This project explores improvements to MobileBERTâ€”a lightweight, task-agnostic BERT model optimized for mobile and resource-constrained environments. Our goal is to boost performance and robustness by applying dynamic data augmentation and efficient fine-tuning using LoRA (Low-Rank Adaptation).

Key Features
ğŸ§  MobileBERT Backbone: Fast and compact transformer model.

ğŸ” Dynamic Augmentation: Real-time text perturbations (word swaps, synonyms, typos) during training.

ğŸ”§ LoRA Fine-Tuning: Efficient weight adaptation without full model retraining.

ğŸ“Š Multi-Task Evaluation: Supports all GLUE tasks with F1 score tracking.

ğŸ“± Mobile-Ready: Ideal for deployment on edge and mobile devices.

Project Goals
Improve generalization, robustness to input variations, and reduce computational costs during trainingâ€”all while preserving MobileBERTâ€™s speed and compactness.

